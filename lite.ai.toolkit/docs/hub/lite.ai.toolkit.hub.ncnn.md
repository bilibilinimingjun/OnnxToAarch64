# Lite.AI.ToolKit.Hub.NCNN  

You can download all the pretrained models files of NCNN format from ([Baidu Drive](https://pan.baidu.com/s/1hlnqyNsFbMseGFWscgVhgQ) code: sc7f)

## Object Detection.

<div id="lite.ai.toolkit.hub.ncnn-object-detection"></div>

|                              Class                              |                  Pretrained NCNN Files                   |            Rename or Converted From (Repo)             | Size  |
|:---------------------------------------------------------------:|:--------------------------------------------------------:|:------------------------------------------------------:|:-----:|
|               *lite::ncnn::cv::detection::YoloV5*               |                  yolov5l.opt.param&bin                   |    [yolov5](https://github.com/ultralytics/yolov5)     | 188Mb |
|               *lite::ncnn::cv::detection::YoloV5*               |                  yolov5m.opt.param&bin                   |    [yolov5](https://github.com/ultralytics/yolov5)     | 85Mb  |
|               *lite::ncnn::cv::detection::YoloV5*               |                  yolov5s.opt.param&bin                   |    [yolov5](https://github.com/ultralytics/yolov5)     | 29Mb  |
|               *lite::ncnn::cv::detection::YoloV5*               |                  yolov5x.opt.param&bin                   |    [yolov5](https://github.com/ultralytics/yolov5)     | 351Mb | 
|               *lite::ncnn::cv::detection::YoloX*                |                  yolox_x.opt.param&bin                   | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 378Mb |
|               *lite::ncnn::cv::detection::YoloX*                |                  yolox_l.opt.param&bin                   | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 207Mb |
|               *lite::ncnn::cv::detection::YoloX*                |                  yolox_m.opt.param&bin                   | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 97Mb  |
|               *lite::ncnn::cv::detection::YoloX*                |                  yolox_s.opt.param&bin                   | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 34Mb  |
|               *lite::ncnn::cv::detection::YoloX*                |                 yolox_tiny.opt.param&bin                 | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 19Mb  |
|               *lite::ncnn::cv::detection::YoloX*                |                 yolox_nano.opt.param&bin                 | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 3.5Mb |
|               *lite::ncnn::cv::detection::YOLOP*                |               yolop-640-640.opt.param&bin                |        [YOLOP](https://github.com/hustvl/YOLOP)        | 30Mb  |
|              *lite::ncnn::cv::detection::NanoDet*               |               nanodet_m_0.5x-opt.param&bin               |     [nanodet](https://github.com/RangiLyu/nanodet)     | 1.1Mb |
|              *lite::ncnn::cv::detection::NanoDet*               |                 nanodet_m-opt.param&bin                  |     [nanodet](https://github.com/RangiLyu/nanodet)     | 3.6Mb |
|              *lite::ncnn::cv::detection::NanoDet*               |               nanodet_m_1.5x-opt.param&bin               |     [nanodet](https://github.com/RangiLyu/nanodet)     | 7.9Mb |
|              *lite::ncnn::cv::detection::NanoDet*               |             nanodet_m_1.5x_416-opt.param&bin             |     [nanodet](https://github.com/RangiLyu/nanodet)     | 7.9Mb |
|              *lite::ncnn::cv::detection::NanoDet*               |               nanodet_m_416-opt.param&bin                |     [nanodet](https://github.com/RangiLyu/nanodet)     | 3.6Mb |
|              *lite::ncnn::cv::detection::NanoDet*               |                 nanodet_g-opt.param&bin                  |     [nanodet](https://github.com/RangiLyu/nanodet)     | 14Mb  |
|              *lite::ncnn::cv::detection::NanoDet*               |                 nanodet_t-opt.param&bin                  |     [nanodet](https://github.com/RangiLyu/nanodet)     | 5.1Mb |
|              *lite::ncnn::cv::detection::NanoDet*               |           nanodet-RepVGG-A0_416-opt.param&bin            |     [nanodet](https://github.com/RangiLyu/nanodet)     | 26Mb  |
|      *lite::ncnn::cv::detection::NanoDetEfficientNetLite*       |       nanodet-EfficientNet-Lite0_320-opt.param&bin       |     [nanodet](https://github.com/RangiLyu/nanodet)     | 12Mb  |
|      *lite::ncnn::cv::detection::NanoDetEfficientNetLite*       |       nanodet-EfficientNet-Lite1_416-opt.param&bin       |     [nanodet](https://github.com/RangiLyu/nanodet)     | 15Mb  |
|      *lite::ncnn::cv::detection::NanoDetEfficientNetLite*       |       nanodet-EfficientNet-Lite2_512-opt.param&bin       |     [nanodet](https://github.com/RangiLyu/nanodet)     | 18Mb  |
|         *lite::ncnn::cv::detection::NanoDetDepreciated*         |         nanodet_m_0.5x-depreciated-opt.param&bin         |     [nanodet](https://github.com/RangiLyu/nanodet)     | 1.1Mb |
|         *lite::ncnn::cv::detection::NanoDetDepreciated*         |           nanodet_m-depreciated-opt.param&bin            |     [nanodet](https://github.com/RangiLyu/nanodet)     | 3.6Mb |
|         *lite::ncnn::cv::detection::NanoDetDepreciated*         |         nanodet_m_1.5x-depreciated-opt.param&bin         |     [nanodet](https://github.com/RangiLyu/nanodet)     | 7.9Mb |
|         *lite::ncnn::cv::detection::NanoDetDepreciated*         |       nanodet_m_1.5x_416-depreciated-opt.param&bin       |     [nanodet](https://github.com/RangiLyu/nanodet)     | 7.9Mb |
|         *lite::ncnn::cv::detection::NanoDetDepreciated*         |         nanodet_m_416-depreciated-opt.param&bin          |     [nanodet](https://github.com/RangiLyu/nanodet)     | 3.6Mb |
|         *lite::ncnn::cv::detection::NanoDetDepreciated*         |           nanodet_g-depreciated-opt.param&bin            |     [nanodet](https://github.com/RangiLyu/nanodet)     | 14Mb  |
|         *lite::ncnn::cv::detection::NanoDetDepreciated*         |           nanodet_t-depreciated-opt.param&bin            |     [nanodet](https://github.com/RangiLyu/nanodet)     | 5.1Mb |
|         *lite::ncnn::cv::detection::NanoDetDepreciated*         |     nanodet-RepVGG-A0_416-depreciated-opt.param&bin      |     [nanodet](https://github.com/RangiLyu/nanodet)     | 26Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLiteDepreciated* | nanodet-EfficientNet-Lite0_320-depreciated-opt.param&bin |     [nanodet](https://github.com/RangiLyu/nanodet)     | 12Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLiteDepreciated* | nanodet-EfficientNet-Lite1_416-depreciated-opt.param&bin |     [nanodet](https://github.com/RangiLyu/nanodet)     | 15Mb  |
| *lite::ncnn::cv::detection::NanoDetEfficientNetLiteDepreciated* | nanodet-EfficientNet-Lite2_512-depreciated-opt.param&bin |     [nanodet](https://github.com/RangiLyu/nanodet)     | 18Mb  |
|           *lite::ncnn::cv::detection::YoloX_V_0_1_1*            |               yolox_x_v0.1.1.opt.param&bin               | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 378Mb |
|           *lite::ncnn::cv::detection::YoloX_V_0_1_1*            |               yolox_l_v0.1.1.opt.param&bin               | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 207Mb |
|           *lite::ncnn::cv::detection::YoloX_V_0_1_1*            |               yolox_m_v0.1.1.opt.param&bin               | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 97Mb  |
|           *lite::ncnn::cv::detection::YoloX_V_0_1_1*            |               yolox_s_v0.1.1.opt.param&bin               | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 34Mb  |
|           *lite::ncnn::cv::detection::YoloX_V_0_1_1*            |             yolox_tiny_v0.1.1.opt.param&bin              | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 19Mb  |
|           *lite::ncnn::cv::detection::YoloX_V_0_1_1*            |             yolox_nano_v0.1.1.opt.param&bin              | [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) | 3.5Mb |
|               *lite::ncnn::cv::detection::YoloR*                |              yolor-p6-320-320.opt.param&bin              |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb |
|               *lite::ncnn::cv::detection::YoloR*                |              yolor-p6-640-640.opt.param&bin              |      [yolor](https://github.com/WongKinYiu/yolor)      | 157Mb |
|               *lite::ncnn::cv::detection::YoloR*                |           yolor-ssss-s2d-640-640.opt.param&bin           |      [yolor](https://github.com/WongKinYiu/yolor)      | 50Mb  |
|            *lite::ncnn::cv::detection::YoloV5_V_6_0*            |           yolov5m.640-640.v.6.0.opt.param&bin            |    [yolov5](https://github.com/ultralytics/yolov5)     | 81Mb  |
|            *lite::ncnn::cv::detection::YoloV5_V_6_0*            |           yolov5s.640-640.v.6.0.opt.param&bin            |    [yolov5](https://github.com/ultralytics/yolov5)     | 28Mb  |
|            *lite::ncnn::cv::detection::YoloV5_V_6_0*            |           yolov5n.640-640.v.6.0.opt.param&bin            |    [yolov5](https://github.com/ultralytics/yolov5)     | 7.5Mb |
|          *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*           |           yolov5m6.640-640.v.6.0.opt.param&bin           |    [yolov5](https://github.com/ultralytics/yolov5)     | 128Mb |
|          *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*           |           yolov5s6.640-640.v.6.0.opt.param&bin           |    [yolov5](https://github.com/ultralytics/yolov5)     | 50Mb  |
|          *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*           |           yolov5n6.640-640.v.6.0.opt.param&bin           |    [yolov5](https://github.com/ultralytics/yolov5)     | 14Mb  |
|          *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*           |          yolov5m6.1280-1280.v.6.0.opt.param&bin          |    [yolov5](https://github.com/ultralytics/yolov5)     | 128Mb |
|          *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*           |          yolov5s6.1280-1280.v.6.0.opt.param&bin          |    [yolov5](https://github.com/ultralytics/yolov5)     | 50Mb  |
|          *lite::ncnn::cv::detection::YoloV5_V_6_0_P6*           |          yolov5n6.1280-1280.v.6.0.opt.param&bin          |    [yolov5](https://github.com/ultralytics/yolov5)     | 14Mb  |
|            *lite::ncnn::cv::detection::NanoDetPlus*             |             nanodet-plus-m_320.opt.param&bin             |     [nanodet](https://github.com/RangiLyu/nanodet)     | 4.5Mb |
|            *lite::ncnn::cv::detection::NanoDetPlus*             |             nanodet-plus-m_416.opt.param&bin             |     [nanodet](https://github.com/RangiLyu/nanodet)     | 4.5Mb |
|            *lite::ncnn::cv::detection::NanoDetPlus*             |          nanodet-plus-m-1.5x_320.opt.param&bin           |     [nanodet](https://github.com/RangiLyu/nanodet)     | 9.4Mb |
|            *lite::ncnn::cv::detection::NanoDetPlus*             |          nanodet-plus-m-1.5x_416.opt.param&bin           |     [nanodet](https://github.com/RangiLyu/nanodet)     | 9.4Mb |
|               *lite::ncnn::cv::detection::YOLOv6*               |          yolov6n-320x320-for-ncnn.opt.param&bin          |      [YOLOv6](https://github.com/meituan/YOLOv6)       | 17Mb  |
|               *lite::ncnn::cv::detection::YOLOv6*               |          yolov6n-640x640-for-ncnn.opt.param&bin          |      [YOLOv6](https://github.com/meituan/YOLOv6)       | 17Mb  |
|               *lite::ncnn::cv::detection::YOLOv6*               |          yolov6s-320x320-for-ncnn.opt.param&bin          |      [YOLOv6](https://github.com/meituan/YOLOv6)       | 66Mb  |
|               *lite::ncnn::cv::detection::YOLOv6*               |          yolov6n-640x640-for-ncnn.opt.param&bin          |      [YOLOv6](https://github.com/meituan/YOLOv6)       | 66Mb  |
|               *lite::ncnn::cv::detection::YOLOv6*               |          yolov6t-640x640-for-ncnn.opt.param&bin          |      [YOLOv6](https://github.com/meituan/YOLOv6)       | 57Mb  |



## Matting.

<div id="lite.ai.toolkit.hub.ncnn-matting"></div>

|                     Class                     |            Pretrained NCNN Files             |                   Rename or Converted From (Repo)                    | Size |
|:---------------------------------------------:|:--------------------------------------------:|:--------------------------------------------------------------------:|:----:|
| *lite::ncnn::cv::matting::RobustVideoMatting* |      rvm_mobilenetv3_fp32-opt.param&bin      | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |  rvm_mobilenetv3_fp32-480-480-opt.param&bin  | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |  rvm_mobilenetv3_fp32-480-640-opt.param&bin  | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |  rvm_mobilenetv3_fp32-640-480-opt.param&bin  | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* | rvm_mobilenetv3_fp32-1080-1920-opt.param&bin | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 14Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |       rvm_resnet50_fp32-opt.param&bin        | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-480-opt.param&bin    | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-480-640-opt.param&bin    | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |   rvm_resnet50_fp32-640-480-opt.param&bin    | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |
| *lite::ncnn::cv::matting::RobustVideoMatting* |  rvm_resnet50_fp32-1080-1920-opt.param&bin   | [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) | 50Mb |

## Face Recognition. 

<div id="lite.ai.toolkit.hub.ncnn-face-recognition"></div>  


|                      Class                      |                      Pretrained NCNN Files                       |                    Rename or Converted From (Repo)                     | Size  |
|:-----------------------------------------------:|:----------------------------------------------------------------:|:----------------------------------------------------------------------:|:-----:|
|     *lite::ncnn::cv::faceid::GlintArcFace*      |                ms1mv3_arcface_r100.opt.param&bin                 |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|     *lite::ncnn::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r50.opt.param&bin                 |       [insightface](https://github.com/deepinsight/insightface)        | 166Mb |
|     *lite::ncnn::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r34.opt.param&bin                 |       [insightface](https://github.com/deepinsight/insightface)        | 130Mb |
|     *lite::ncnn::cv::faceid::GlintArcFace*      |                 ms1mv3_arcface_r18.opt.param&bin                 |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|     *lite::ncnn::cv::faceid::GlintCosFace*      |               glint360k_cosface_r100.opt.param&bin               |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|     *lite::ncnn::cv::faceid::GlintCosFace*      |               glint360k_cosface_r50.opt.param&bin                |       [insightface](https://github.com/deepinsight/insightface)        | 166Mb |
|     *lite::ncnn::cv::faceid::GlintCosFace*      |               glint360k_cosface_r34.opt.param&bin                |       [insightface](https://github.com/deepinsight/insightface)        | 130Mb |
|     *lite::ncnn::cv::faceid::GlintCosFace*      |               glint360k_cosface_r18.opt.param&bin                |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|    *lite::ncnn::cv::faceid::GlintPartialFC*     |             partial_fc_glint360k_r100.opt.param&bin              |       [insightface](https://github.com/deepinsight/insightface)        | 248Mb |
|    *lite::ncnn::cv::faceid::GlintPartialFC*     |              partial_fc_glint360k_r50.opt.param&bin              |       [insightface](https://github.com/deepinsight/insightface)        | 91Mb  |
|        *lite::ncnn::cv::faceid::FaceNet*        |              facenet_vggface2_resnet.opt.param&bin               |       [facenet...](https://github.com/timesler/facenet-pytorch)        | 89Mb  |
|        *lite::ncnn::cv::faceid::FaceNet*        |            facenet_casia-webface_resnet.opt.param&bin            |       [facenet...](https://github.com/timesler/facenet-pytorch)        | 89Mb  |
|     *lite::ncnn::cv::faceid::FocalArcFace*      |              focal-arcface-ms1m-ir152.opt.param&bin              |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 269Mb |
|     *lite::ncnn::cv::faceid::FocalArcFace*      |          focal-arcface-ms1m-ir50-epoch120.opt.param&bin          |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
|     *lite::ncnn::cv::faceid::FocalArcFace*      |          focal-arcface-ms1m-ir50-epoch63.opt.param&bin           |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
|   *lite::ncnn::cv::faceid::FocalAsiaArcFace*    |             focal-arcface-bh-ir50-asia.opt.param&bin             |   [face.evoLVe...](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch)   | 166Mb |
| *lite::ncnn::cv::faceid::TencentCurricularFace* |          Tencent_CurricularFace_Backbone.opt.param&bin           |               [TFace](https://github.com/Tencent/TFace)                | 249Mb |
|    *lite::ncnn::cv::faceid::TencentCifpFace*    |        Tencent_Cifp_BUPT_Balancedface_IR_34.opt.param&bin        |               [TFace](https://github.com/Tencent/TFace)                | 130Mb |
|    *lite::ncnn::cv::faceid::CenterLossFace*     |              CenterLossFace_epoch_100.opt.param&bin              |   [center-loss...](https://github.com/louis-she/center-loss.pytorch)   | 280Mb |
|      *lite::ncnn::cv::faceid::SphereFace*       |                 sphere20a_20171020.opt.param&bin                 |      [sphere...](https://github.com/clcarwin/sphereface_pytorch)       | 86Mb  |
|     *lite::ncnn::cv::faceid:MobileFaceNet*      |             MobileFaceNet_Pytorch_068.opt.param&bin              |   [MobileFace...](https://github.com/Xiaoccer/MobileFaceNet_Pytorch)   | 3.8Mb |
|    *lite::ncnn::cv::faceid:CavaGhostArcFace*    |      cavaface_GhostNet_x1.3_Arcface_Epoch_24.opt.param&bin       |     [cavaface...](https://github.com/cavalleria/cavaface.pytorch)      | 15Mb  |
|    *lite::ncnn::cv::faceid:CavaCombinedFace*    |        cavaface_IR_SE_100_Combined_Epoch_24.opt.param&bin        |     [cavaface...](https://github.com/cavalleria/cavaface.pytorch)      | 250Mb |
|   *lite::ncnn::cv::faceid:MobileSEFocalFace*    | face_recognition.pytorch_Mobilenet_se_focal_121000.opt.param&bin | [face_recog...](https://github.com/grib0ed0v/face_recognition.pytorch) | 4.5Mb |


## Face Detection.

<div id="lite.ai.toolkit.hub.ncnn-face-detection"></div>  

|                    Class                    |                Pretrained NCNN Files                 |                             Rename or Converted From (Repo)                             |  Size  |
|:-------------------------------------------:|:----------------------------------------------------:|:---------------------------------------------------------------------------------------:|:------:|
|  *lite::ncnn::cv::face::detect::UltraFace*  |             ultraface-rfb-320.param&bin              | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.5Mb  |
|  *lite::ncnn::cv::face::detect::UltraFace*  |             ultraface-slim-320.param&bin             | [Ultra-Light...](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | 1.2Mb  |
| *lite::ncnn::cv::face::detect::RetinaFace*  |     Pytorch_RetinaFace_mobile0.25.opt.param&bin      |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::ncnn::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-640-640.opt.param&bin  |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::ncnn::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-320-320.opt.param&bin  |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
| *lite::ncnn::cv::face::detect::RetinaFace*  | Pytorch_RetinaFace_mobile0.25-720-1080.opt.param&bin |             [...Retinaface](https://github.com/biubug6/Pytorch_Retinaface)              | 1.6Mb  |
|  *lite::ncnn::cv::face::detect::FaceBoxes*  |               FaceBoxes.opt.param&bin                |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::ncnn::cv::face::detect::FaceBoxes*  |           FaceBoxes-640-640.opt.param&bin            |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::ncnn::cv::face::detect::FaceBoxes*  |           FaceBoxes-320-320.opt.param&bin            |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|  *lite::ncnn::cv::face::detect::FaceBoxes*  |           FaceBoxes-720-1080.opt.param&bin           |                [FaceBoxes](https://github.com/zisianw/FaceBoxes.PyTorch)                | 3.8Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |        scrfd_500m_shape160x160.opt.param&bin         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |        scrfd_500m_shape320x320.opt.param&bin         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |        scrfd_500m_shape640x640.opt.param&bin         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape160x160.opt.param&bin      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::ncnn::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape320x320.opt.param&bin      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::ncnn::cv::face::detect::SCRFD*    |     scrfd_500m_bnkps_shape640x640.opt.param&bin      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.5Mb  |  
|    *lite::ncnn::cv::face::detect::SCRFD*    |         scrfd_1g_shape160x160.opt.param&bin          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |         scrfd_1g_shape320x320.opt.param&bin          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |         scrfd_1g_shape640x640.opt.param&bin          |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 2.7Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape160x160.opt.param&bin         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape320x320.opt.param&bin         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |        scrfd_2.5g_shape640x640.opt.param&bin         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |
|    *lite::ncnn::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape160x160.opt.param&bin      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::ncnn::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape320x320.opt.param&bin      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::ncnn::cv::face::detect::SCRFD*    |     scrfd_2.5g_bnkps_shape640x640.opt.param&bin      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 3.3Mb  |  
|    *lite::ncnn::cv::face::detect::SCRFD*    |         scrfd_10g_shape640x640.opt.param&bin         |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |
|    *lite::ncnn::cv::face::detect::SCRFD*    |        scrfd_10g_shape1280x1280.opt.param&bin        |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |
|    *lite::ncnn::cv::face::detect::SCRFD*    |      scrfd_10g_bnkps_shape640x640.opt.param&bin      |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |  
|    *lite::ncnn::cv::face::detect::SCRFD*    |     scrfd_10g_bnkps_shape1280x1280.opt.param&bin     |     [SCRFD](https://github.com/deepinsight/insightface/blob/master/detection/scrfd)     | 16.9Mb |  
|  *lite::ncnn::cv::face::detect::YOLO5Face*  |          yolov5face-l-640x640.opt.param&bin          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 181Mb  |
|  *lite::ncnn::cv::face::detect::YOLO5Face*  |          yolov5face-m-640x640.opt.param&bin          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  |  83Mb  |
|  *lite::ncnn::cv::face::detect::YOLO5Face*  |        yolov5face-n-0.5-320x320.opt.param&bin        |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 2.5Mb  |
|  *lite::ncnn::cv::face::detect::YOLO5Face*  |        yolov5face-n-0.5-640x640.opt.param&bin        |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 4.6Mb  |
|  *lite::ncnn::cv::face::detect::YOLO5Face*  |          yolov5face-n-640x640.opt.param&bin          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  | 9.5Mb  |
|  *lite::ncnn::cv::face::detect::YOLO5Face*  |          yolov5face-s-640x640.opt.param&bin          |                 [YOLO5Face](https://github.com/deepcam-cn/yolov5-face)                  |  30Mb  |
| *lite::ncnn::cv::face::detect::FaceBoxesV2* |          faceboxesv2-640x640.opt.param&bin           |                [FaceBoxesV2](https://github.com/jhb86253817/FaceBoxesV2)                | 4.0Mb  |


## Face Alignment.

<div id="lite.ai.toolkit.hub.ncnn-face-alignment"></div>  


|                      Class                       |                         Pretrained NCNN Files                          |                  Rename or Converted From (Repo)                   |  Size   |
|:------------------------------------------------:|:----------------------------------------------------------------------:|:------------------------------------------------------------------:|:-------:|
|       *lite::ncnn::cv::face::align::PFLD*        |                      pfld-106-lite.opt.param&bin                       | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  1.0Mb  |
|       *lite::ncnn::cv::face::align::PFLD*        |                       pfld-106-v3.opt.param&bin                        | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  5.5Mb  |
|       *lite::ncnn::cv::face::align::PFLD*        |                       pfld-106-v2.opt.param&bin                        | [pfld_106_...](https://github.com/Hsintao/pfld_106_face_landmarks) |  5.0Mb  |
|      *lite::ncnn::cv::face::align::PFLD98*       |                    PFLD-pytorch-pfld.opt.param&bin                     |       [PFLD...](https://github.com/polarisZhao/PFLD-pytorch)       |  4.8Mb  |
|   *lite::ncnn::cv::face::align::MobileNetV268*   |       pytorch_face_landmarks_landmark_detection_56.opt.param&bin       |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  9.4Mb  |
|  *lite::ncnn::cv::face::align::MobileNetV2SE68*  | pytorch_face_landmarks_landmark_detection_56_se_external.opt.param&bin |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  11Mb   |
|      *lite::ncnn::cv::face::align::PFLD68*       |               pytorch_face_landmarks_pfld.opt.param&bin                |  [...landmark](https://github.com/cunjian/pytorch_face_landmark)   |  2.8Mb  |
| *lite::ncnn::cv::face::align::FaceLandmarks1000* |                     FaceLandmark1000.opt.param&bin                     |   [FaceLandm...](https://github.com/Single430/FaceLandmark1000)    |  2.0Mb  |
|     *lite::ncnn::cv::face::align::PIPNet98*      |            pipnet_resnet18_10x98x32x256_wflw.opt.param&bin             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::ncnn::cv::face::align::PIPNet68*      |            pipnet_resnet18_10x68x32x256_300w.opt.param&bin             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::ncnn::cv::face::align::PIPNet29*      |            pipnet_resnet18_10x29x32x256_cofw.opt.param&bin             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::ncnn::cv::face::align::PIPNet19*      |            pipnet_resnet18_10x19x32x256_aflw.opt.param&bin             |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 44.0Mb  |
|     *lite::ncnn::cv::face::align::PIPNet98*      |            pipnet_resnet101_10x98x32x256_wflw.opt.param&bin            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::ncnn::cv::face::align::PIPNet68*      |            pipnet_resnet101_10x68x32x256_300w.opt.param&bin            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::ncnn::cv::face::align::PIPNet29*      |            pipnet_resnet101_10x29x32x256_cofw.opt.param&bin            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |
|     *lite::ncnn::cv::face::align::PIPNet19*      |            pipnet_resnet101_10x19x32x256_aflw.opt.param&bin            |          [PIPNet](https://github.com/jhb86253817/PIPNet)           | 150.0Mb |


## Face Attributes.

<div id="lite.ai.toolkit.hub.ncnn-face-attributes"></div>  


|                      Class                      |                   Pretrained NCNN Files                    |                      Rename or Converted From (Repo)                      | Size |
|:-----------------------------------------------:|:----------------------------------------------------------:|:-------------------------------------------------------------------------:|:----:|
|   *lite::ncnn::cv::face::attr::AgeGoogleNet*    |                age_googlenet.opt.param&bin                 |               [onnx-models](https://github.com/onnx/models)               | 23Mb |
|  *lite::ncnn::cv::face::attr::GenderGoogleNet*  |               gender_googlenet.opt.param&bin               |               [onnx-models](https://github.com/onnx/models)               | 23Mb |
|  *lite::ncnn::cv::face::attr::EmotionFerPlus*   |              emotion-ferplus-7.opt.param&bin               |               [onnx-models](https://github.com/onnx/models)               | 33Mb |
|  *lite::ncnn::cv::face::attr::EmotionFerPlus*   |              emotion-ferplus-8.opt.param&bin               |               [onnx-models](https://github.com/onnx/models)               | 33Mb |
| *lite::ncnn::cv::face::attr::EfficientEmotion7* |      face-emotion-recognition-enet_b0_7.opt.param&bin      | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
| *lite::ncnn::cv::face::attr::EfficientEmotion8* | face-emotion-recognition-enet_b0_8_best_afew.opt.param&bin | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
| *lite::ncnn::cv::face::attr::EfficientEmotion8* | face-emotion-recognition-enet_b0_8_best_vgaf.opt.param&bin | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 15Mb |
|  *lite::ncnn::cv::face::attr::MobileEmotion7*   |     face-emotion-recognition-mobilenet_7.opt.param&bin     | [face-emo...](https://github.com/HSE-asavchenko/face-emotion-recognition) | 13Mb |


## Classification.

<div id="lite.ai.toolkit.hub.ncnn-classification"></div>


|                     Class                      |           Pretrained NCNN Files           |               Rename or Converted From (Repo)                | Size  |
|:----------------------------------------------:|:-----------------------------------------:|:------------------------------------------------------------:|:-----:|
| *lite::ncnn::cv::classification::ShuffleNetV2* |      shufflenet-v2-10.opt.param&bin       |        [onnx-models](https://github.com/onnx/models)         | 8.7Mb |
| *lite::ncnn::cv::classification::DenseNet121*  |         densenet121.opt.param&bin         |       [torchvision](https://github.com/pytorch/vision)       | 30Mb  |
|   *lite::ncnn::cv::classification::GhostNet*   |          ghostnet.opt.param&bin           |       [torchvision](https://github.com/pytorch/vision)       | 20Mb  |
|   *lite::ncnn::cv::classification::HdrDNet*    |           hardnet.opt.param&bin           |       [torchvision](https://github.com/pytorch/vision)       | 13Mb  |
|    *lite::ncnn::cv::classification::IBNNet*    |          ibnnet18.opt.param&bin           |       [torchvision](https://github.com/pytorch/vision)       | 97Mb  |
| *lite::ncnn::cv::classification::MobileNetV2*  |         mobilenetv2.opt.param&bin         |       [torchvision](https://github.com/pytorch/vision)       | 13Mb  |
|    *lite::ncnn::cv::classification::ResNet*    |          resnet18.opt.param&bin           |       [torchvision](https://github.com/pytorch/vision)       | 44Mb  |
|   *lite::ncnn::cv::classification::ResNeXt*    |           resnext.opt.param&bin           |       [torchvision](https://github.com/pytorch/vision)       | 95Mb  |
|   *lite::ncnn::cv::classification::InsectID*   | quarrying_insect_identifier.opt.param&bin | [InsectID](https://github.com/quarrying/quarrying-insect-id) | 27Mb  |
|    *lite::ncnn::cv::classification:PlantID*    |   quarrying_plantid_model.opt.param&bin   |  [PlantID](https://github.com/quarrying/quarrying-plant-id)  | 30Mb  |

## Segmentation.

<div id="lite.ai.toolkit.hub.ncnn-segmentation"></div>  


|                       Class                        |         Pretrained NCNN Files          |                      Rename or Converted From (Repo)                       | Size  |
|:--------------------------------------------------:|:--------------------------------------:|:--------------------------------------------------------------------------:|:-----:|
| *lite::ncnn::cv::segmentation::DeepLabV3ResNet101* | deeplabv3_resnet101_coco.opt.param&bin |              [torchvision](https://github.com/pytorch/vision)              | 232Mb |
|    *lite::ncnn::cv::segmentation::FCNResNet101*    |      fcn_resnet101.opt.param&bin       |              [torchvision](https://github.com/pytorch/vision)              | 207Mb |
| *lite::ncnn::cv::segmentation::FaceParsingBiSeNet* |   face_parsing_512x512.opt.param&bin   | [face-parsing.PyTorch](https://github.com/zllrunning/face-parsing.PyTorch) |  50M  |
| *lite::ncnn::cv::segmentation::FaceParsingBiSeNet* |  face_parsing_1024x1024.opt.param&bin  | [face-parsing.PyTorch](https://github.com/zllrunning/face-parsing.PyTorch) |  50M  |


## Style Transfer.

<div id="lite.ai.toolkit.hub.ncnn-style-transfer"></div>  

|                   Class                    |                    Pretrained NCNN Files                     |        Rename or Converted From (Repo)        | Size  |
|:------------------------------------------:|:------------------------------------------------------------:|:---------------------------------------------:|:-----:|
| *lite::ncnn::cv::style::FastStyleTransfer* |                 style-mosaic-8.opt.param&bin                 | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |                 style-candy-9.opt.param&bin                  | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |                 style-udnie-8.opt.param&bin                  | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |                 style-udnie-9.opt.param&bin                  | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |               style-pointilism-8.opt.param&bin               | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |               style-pointilism-9.opt.param&bin               | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |             style-rain-princess-9.opt.param&bin              | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |             style-rain-princess-8.opt.param&bin              | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |                 style-candy-8.opt.param&bin                  | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
| *lite::ncnn::cv::style::FastStyleTransfer* |                 style-mosaic-9.opt.param&bin                 | [onnx-models](https://github.com/onnx/models) | 6.4Mb |
|     *lite::ncnn::cv::matting::MODNet*      | modnet_photographic_portrait_matting-1024x1024.opt.param&bin |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      | modnet_photographic_portrait_matting-1024x512.opt.param&bin  |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |  modnet_photographic_portrait_matting-256x256.opt.param&bin  |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |  modnet_photographic_portrait_matting-256x512.opt.param&bin  |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      | modnet_photographic_portrait_matting-512x1024.opt.param&bin  |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |  modnet_photographic_portrait_matting-512x256.opt.param&bin  |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |  modnet_photographic_portrait_matting-512x512.opt.param&bin  |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |    modnet_webcam_portrait_matting-1024x1024.opt.param&bin    |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |    modnet_webcam_portrait_matting-1024x512.opt.param&bin     |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |     modnet_webcam_portrait_matting-256x256.opt.param&bin     |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |     modnet_webcam_portrait_matting-256x512.opt.param&bin     |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |    modnet_webcam_portrait_matting-512x1024.opt.param&bin     |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |     modnet_webcam_portrait_matting-512x256.opt.param&bin     |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |
|     *lite::ncnn::cv::matting::MODNet*      |     modnet_webcam_portrait_matting-512x512.opt.param&bin     |  [MODNet](https://github.com/ZHKKKe/MODNet)   | 24Mb  |

